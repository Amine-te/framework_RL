# Reinforcement Learning Framework

A flexible and extensible **Reinforcement Learning (RL)** framework built around a customizable **GridWorld** environment.  
This project provides an experimental playground for developing, testing, and comparing a wide range of RL algorithms in controlled scenarios.

---

## ğŸ“ Project Structure

```
framework_RL/
â”œâ”€â”€ src/                          # Core framework code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ gridworld.py              # GridWorld environment implementation
â”‚
â”œâ”€â”€ scripts/                      # Experiment scripts
â”‚   â”œâ”€â”€ agents_basic.py           # Classic RL agents (PI, VI, MC, QL)
â”‚   â”œâ”€â”€ q_learning.py             # Q-Learning parameter studies
â”‚   â”œâ”€â”€ q_learning_moving_goal.py # Q-Learning with moving goals
â”‚   â”œâ”€â”€ deep_q_learning_fixed_goal.py      # DQN with fixed goals
â”‚   â”œâ”€â”€ deep_q_learning_moving_goal.py     # DQN with moving goals
â”‚   â”œâ”€â”€ deep_q_learning_stable_baseline3.py # Stable-Baselines3 DQN
â”‚   â”œâ”€â”€ func_approx_comparison_no_goal_info.py    # Function approximation (no goal)
â”‚   â”œâ”€â”€ func_approx_camparison_with_goal_info.py  # Function approximation (with goal)
â”‚   â””â”€â”€ test_ql_agents.py         # Test trained agents and generate GIFs
â”‚
â””â”€â”€ results/                      # All outputs (generated by scripts)
    â”œâ”€â”€ agents/                   # Saved trained agents (.pkl files)
    â”œâ”€â”€ plots/                    # Performance plots and visualizations (.png)
    â””â”€â”€ gifs/                     # Agent behavior animations (.gif)
```

---

## ğŸš€ Overview

This framework aims to make it easy to:
- Design and visualize **grid-based environments** with configurable layouts, rewards, and dynamics.
- Implement and experiment with **different RL agents** â€” from classic tabular methods to modern function approximations.
- Compare agent performance across **varied test setups** and visualize their learning behavior using plots, metrics, and GIFs.

It is still **under active development**, with many components and improvements yet to come.

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.7 or higher
- pip (Python package manager)

### Setup

1. **Clone or download this repository**

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

   This will install:
   - `numpy` - Numerical computing
   - `matplotlib` - Plotting and visualization
   - `torch` - PyTorch for deep learning (CPU version by default)
   - `stable-baselines3` - RL algorithms library (required for `deep_q_learning_stable_baseline3.py`)
   - `gymnasium` - Gym-compatible environment interface (optional but recommended)
   - `Pillow` - Image processing for GIF generation (required for `test_ql_agents.py`)

3. **Optional - GPU Support**: 
   
   By default, `requirements.txt` installs **CPU-only PyTorch**. For GPU acceleration:
   
   **Option A**: Install GPU version after installing requirements:
   ```bash
   pip install -r requirements.txt
   # Then install GPU version (replace cu121 with your CUDA version)
   pip install torch --index-url https://download.pytorch.org/whl/cu121
   ```
   
   **Option B**: Install GPU version directly (skip torch from requirements.txt):
   ```bash
   # Install other dependencies first
   pip install numpy matplotlib stable-baselines3 gymnasium Pillow
   
   # Then install PyTorch with GPU support
   # Visit https://pytorch.org/get-started/locally/ to get the correct command
   # Example for CUDA 11.8:
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   ```
   
   **Note**: GPU support requires:
   - NVIDIA GPU with CUDA support
   - CUDA toolkit installed on your system
   - Matching CUDA version between your system and PyTorch

### Quick Start

Run a simple experiment:
```bash
python scripts/agents_basic.py --agent QL --episodes 1000
```

---

## ğŸ§  Features

- **Customizable Environments**  
  - Adjustable grid size, obstacles, and goal positions.  
  - Dynamic scenarios (e.g., moving or changing goals).  

- **Agents & Algorithms**  
  - Value-based methods: *Value Iteration, Policy Iteration*.  
  - Model-free methods: *Monte Carlo, Q-Learning, SARSA*.  
  - Deep RL: *DQN, Function Approximators, and beyond*.  

- **Evaluation & Visualization**  
  - Comparative experiments between agents.  
  - Performance metrics, convergence plots, and episode summaries.  
  - Animated trajectories and learning process GIFs.  

---

## ğŸ—ºï¸ Navigation Guide

### Getting Started

1. **Core Environment**: The `src/gridworld.py` module contains the `GridWorldEnv` class - the foundation of all experiments.

2. **Running Experiments**: All experiment scripts are in the `scripts/` directory. Run them from the project root:
   ```bash
   python scripts/q_learning.py --episodes 2000
   python scripts/agents_basic.py --agent all
   ```

3. **Viewing Results**: 
   - **Trained Agents**: Saved in `results/agents/` as `.pkl` files
   - **Plots**: Performance visualizations saved in `results/plots/`
   - **GIFs**: Agent behavior animations in `results/gifs/`

### Experiment Scripts

- **`agents_basic.py`**: Compare classic RL algorithms (Policy Iteration, Value Iteration, Monte Carlo, Q-Learning)
- **`q_learning.py`**: Parameter studies for Q-Learning (alpha, gamma, grid size)
- **`q_learning_moving_goal.py`**: Q-Learning with dynamically changing goals
- **`deep_q_learning_*.py`**: Deep Q-Network implementations (naive, target network, Stable-Baselines3)
- **`func_approx_*.py`**: Function approximation comparisons
- **`test_ql_agents.py`**: Load trained agents and generate GIF visualizations

### Output Locations

All scripts automatically save their outputs to the `results/` directory:
- Trained agents â†’ `results/agents/`
- Plots and charts â†’ `results/plots/`
- GIF animations â†’ `results/gifs/`

---
